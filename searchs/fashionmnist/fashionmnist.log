04/26 01:42:38 PM | 
04/26 01:42:38 PM | Parameters:
04/26 01:42:38 PM | ALPHA_LR=0.0003
04/26 01:42:38 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 01:42:38 PM | BATCH_SIZE=64
04/26 01:42:38 PM | DATA_PATH=./data/
04/26 01:42:38 PM | DATASET=fashionmnist
04/26 01:42:38 PM | EPOCHS=50
04/26 01:42:38 PM | GPUS=[0]
04/26 01:42:38 PM | INIT_CHANNELS=16
04/26 01:42:38 PM | LAYERS=8
04/26 01:42:38 PM | NAME=fashionmnist
04/26 01:42:38 PM | PATH=searchs/fashionmnist
04/26 01:42:38 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 01:42:38 PM | PRINT_FREQ=50
04/26 01:42:38 PM | SEED=2
04/26 01:42:38 PM | W_GRAD_CLIP=5.0
04/26 01:42:38 PM | W_LR=0.025
04/26 01:42:38 PM | W_LR_MIN=0.001
04/26 01:42:38 PM | W_MOMENTUM=0.9
04/26 01:42:38 PM | W_WEIGHT_DECAY=0.0003
04/26 01:42:38 PM | WORKERS=2
04/26 01:42:38 PM | 
04/26 01:42:38 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 01:42:48 PM | Train: [ 1/50] Step 000/468 Loss 2.358 Prec@(1,5) (15.6%, 57.8%)
04/26 01:45:09 PM | Train: [ 1/50] Step 050/468 Loss 1.223 Prec@(1,5) (53.9%, 94.6%)
04/26 01:45:52 PM | 
04/26 01:45:52 PM | Parameters:
04/26 01:45:52 PM | ALPHA_LR=0.0003
04/26 01:45:52 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 01:45:52 PM | BATCH_SIZE=64
04/26 01:45:52 PM | DATA_PATH=./data/
04/26 01:45:52 PM | DATASET=fashionmnist
04/26 01:45:52 PM | EPOCHS=50
04/26 01:45:52 PM | GPUS=[0]
04/26 01:45:52 PM | INIT_CHANNELS=16
04/26 01:45:52 PM | LAYERS=8
04/26 01:45:52 PM | NAME=fashionmnist
04/26 01:45:52 PM | PATH=searchs/fashionmnist
04/26 01:45:52 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 01:45:52 PM | PRINT_FREQ=50
04/26 01:45:52 PM | SEED=2
04/26 01:45:52 PM | W_GRAD_CLIP=5.0
04/26 01:45:52 PM | W_LR=0.025
04/26 01:45:52 PM | W_LR_MIN=0.001
04/26 01:45:52 PM | W_MOMENTUM=0.9
04/26 01:45:52 PM | W_WEIGHT_DECAY=0.0003
04/26 01:45:52 PM | WORKERS=2
04/26 01:45:52 PM | 
04/26 01:45:52 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.0908, 0.0910, 0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0911, 0.0908, 0.0908, 0.0909, 0.0910,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0910, 0.0908,
         0.0909, 0.0909],
        [0.0908, 0.0911, 0.0910, 0.0909, 0.0910, 0.0908, 0.0910, 0.0907, 0.0909,
         0.0909, 0.0909],
        [0.0910, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0910, 0.0911, 0.0908, 0.0909, 0.0908, 0.0909, 0.0909, 0.0910, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0909, 0.0908, 0.0909, 0.0911, 0.0910, 0.0909, 0.0911, 0.0909,
         0.0909, 0.0907],
        [0.0908, 0.0909, 0.0909, 0.0911, 0.0909, 0.0908, 0.0910, 0.0909, 0.0908,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0911, 0.0909, 0.0908, 0.0911, 0.0910, 0.0910, 0.0908, 0.0909, 0.0906,
         0.0909, 0.0910],
        [0.0909, 0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0910, 0.0908, 0.0909, 0.0910, 0.0911, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0910, 0.0910, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0910],
        [0.0909, 0.0908, 0.0908, 0.0910, 0.0908, 0.0910, 0.0909, 0.0909, 0.0911,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.0909, 0.0908, 0.0908, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909, 0.0910,
         0.0911, 0.0910],
        [0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0910, 0.0908, 0.0911, 0.0908,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0910, 0.0909, 0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0910, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909,
         0.0910, 0.0909],
        [0.0909, 0.0909, 0.0907, 0.0911, 0.0909, 0.0909, 0.0910, 0.0910, 0.0909,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0910, 0.0908, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0908, 0.0910,
         0.0909, 0.0909],
        [0.0909, 0.0908, 0.0909, 0.0908, 0.0910, 0.0908, 0.0910, 0.0910, 0.0909,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0908, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0910, 0.0909, 0.0909, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910,
         0.0908, 0.0910],
        [0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0909, 0.0910, 0.0909, 0.0907, 0.0909, 0.0911, 0.0907, 0.0909, 0.0910,
         0.0909, 0.0909],
        [0.0908, 0.0908, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 01:52:42 PM | 
04/26 01:52:42 PM | Parameters:
04/26 01:52:42 PM | ALPHA_LR=0.0003
04/26 01:52:42 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 01:52:42 PM | BATCH_SIZE=32
04/26 01:52:42 PM | DATA_PATH=./data/
04/26 01:52:42 PM | DATASET=fashionmnist
04/26 01:52:42 PM | EPOCHS=50
04/26 01:52:42 PM | GPUS=[0]
04/26 01:52:42 PM | INIT_CHANNELS=16
04/26 01:52:42 PM | LAYERS=8
04/26 01:52:42 PM | NAME=fashionmnist
04/26 01:52:42 PM | PATH=searchs/fashionmnist
04/26 01:52:42 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 01:52:42 PM | PRINT_FREQ=50
04/26 01:52:42 PM | SEED=2
04/26 01:52:42 PM | W_GRAD_CLIP=5.0
04/26 01:52:42 PM | W_LR=0.025
04/26 01:52:42 PM | W_LR_MIN=0.001
04/26 01:52:42 PM | W_MOMENTUM=0.9
04/26 01:52:42 PM | W_WEIGHT_DECAY=0.0003
04/26 01:52:42 PM | WORKERS=2
04/26 01:52:42 PM | 
04/26 01:52:42 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.0908, 0.0910, 0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0911, 0.0908, 0.0908, 0.0909, 0.0910,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0910, 0.0908,
         0.0909, 0.0909],
        [0.0908, 0.0911, 0.0910, 0.0909, 0.0910, 0.0908, 0.0910, 0.0907, 0.0909,
         0.0909, 0.0909],
        [0.0910, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0910, 0.0911, 0.0908, 0.0909, 0.0908, 0.0909, 0.0909, 0.0910, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0909, 0.0908, 0.0909, 0.0911, 0.0910, 0.0909, 0.0911, 0.0909,
         0.0909, 0.0907],
        [0.0908, 0.0909, 0.0909, 0.0911, 0.0909, 0.0908, 0.0910, 0.0909, 0.0908,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0911, 0.0909, 0.0908, 0.0911, 0.0910, 0.0910, 0.0908, 0.0909, 0.0906,
         0.0909, 0.0910],
        [0.0909, 0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0910, 0.0908, 0.0909, 0.0910, 0.0911, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0910, 0.0910, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0910],
        [0.0909, 0.0908, 0.0908, 0.0910, 0.0908, 0.0910, 0.0909, 0.0909, 0.0911,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.0909, 0.0908, 0.0908, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909, 0.0910,
         0.0911, 0.0910],
        [0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0910, 0.0908, 0.0911, 0.0908,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0910, 0.0909, 0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0910, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909,
         0.0910, 0.0909],
        [0.0909, 0.0909, 0.0907, 0.0911, 0.0909, 0.0909, 0.0910, 0.0910, 0.0909,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0910, 0.0908, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0908, 0.0910,
         0.0909, 0.0909],
        [0.0909, 0.0908, 0.0909, 0.0908, 0.0910, 0.0908, 0.0910, 0.0910, 0.0909,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0908, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0910, 0.0909, 0.0909, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910,
         0.0908, 0.0910],
        [0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0909, 0.0910, 0.0909, 0.0907, 0.0909, 0.0911, 0.0907, 0.0909, 0.0910,
         0.0909, 0.0909],
        [0.0908, 0.0908, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 01:53:57 PM | 
04/26 01:53:57 PM | Parameters:
04/26 01:53:57 PM | ALPHA_LR=0.0003
04/26 01:53:57 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 01:53:57 PM | BATCH_SIZE=32
04/26 01:53:57 PM | DATA_PATH=./data/
04/26 01:53:57 PM | DATASET=fashionmnist
04/26 01:53:57 PM | EPOCHS=10
04/26 01:53:57 PM | GPUS=[0]
04/26 01:53:57 PM | INIT_CHANNELS=16
04/26 01:53:57 PM | LAYERS=8
04/26 01:53:57 PM | NAME=fashionmnist
04/26 01:53:57 PM | PATH=searchs/fashionmnist
04/26 01:53:57 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 01:53:57 PM | PRINT_FREQ=50
04/26 01:53:57 PM | SEED=2
04/26 01:53:57 PM | W_GRAD_CLIP=5.0
04/26 01:53:57 PM | W_LR=0.025
04/26 01:53:57 PM | W_LR_MIN=0.001
04/26 01:53:57 PM | W_MOMENTUM=0.9
04/26 01:53:57 PM | W_WEIGHT_DECAY=0.0003
04/26 01:53:57 PM | WORKERS=2
04/26 01:53:57 PM | 
04/26 01:53:57 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.0908, 0.0910, 0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0911, 0.0908, 0.0908, 0.0909, 0.0910,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0910, 0.0908,
         0.0909, 0.0909],
        [0.0908, 0.0911, 0.0910, 0.0909, 0.0910, 0.0908, 0.0910, 0.0907, 0.0909,
         0.0909, 0.0909],
        [0.0910, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0910, 0.0911, 0.0908, 0.0909, 0.0908, 0.0909, 0.0909, 0.0910, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0909, 0.0908, 0.0909, 0.0911, 0.0910, 0.0909, 0.0911, 0.0909,
         0.0909, 0.0907],
        [0.0908, 0.0909, 0.0909, 0.0911, 0.0909, 0.0908, 0.0910, 0.0909, 0.0908,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0911, 0.0909, 0.0908, 0.0911, 0.0910, 0.0910, 0.0908, 0.0909, 0.0906,
         0.0909, 0.0910],
        [0.0909, 0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0910, 0.0908, 0.0909, 0.0910, 0.0911, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0908, 0.0910, 0.0910, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0910],
        [0.0909, 0.0908, 0.0908, 0.0910, 0.0908, 0.0910, 0.0909, 0.0909, 0.0911,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.0909, 0.0908, 0.0908, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909, 0.0910,
         0.0911, 0.0910],
        [0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0910, 0.0908, 0.0911, 0.0908,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0910, 0.0909, 0.0909, 0.0909, 0.0910, 0.0910, 0.0908, 0.0910, 0.0909,
         0.0909, 0.0908],
        [0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0908, 0.0909,
         0.0910, 0.0909],
        [0.0909, 0.0909, 0.0907, 0.0911, 0.0909, 0.0909, 0.0910, 0.0910, 0.0909,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0909, 0.0910, 0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0908],
        [0.0908, 0.0908, 0.0908, 0.0910, 0.0910, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0909, 0.0910],
        [0.0910, 0.0908, 0.0909, 0.0908, 0.0910, 0.0909, 0.0909, 0.0908, 0.0910,
         0.0909, 0.0909],
        [0.0909, 0.0908, 0.0909, 0.0908, 0.0910, 0.0908, 0.0910, 0.0910, 0.0909,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0908, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0910, 0.0909, 0.0909,
         0.0910, 0.0908],
        [0.0909, 0.0910, 0.0909, 0.0909, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910,
         0.0908, 0.0910],
        [0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0908, 0.0909],
        [0.0909, 0.0910, 0.0909, 0.0907, 0.0909, 0.0911, 0.0907, 0.0909, 0.0910,
         0.0909, 0.0909],
        [0.0908, 0.0908, 0.0910, 0.0910, 0.0909, 0.0910, 0.0909, 0.0908, 0.0909,
         0.0910, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 02:01:08 PM | 
04/26 02:01:08 PM | Parameters:
04/26 02:01:08 PM | ALPHA_LR=0.0003
04/26 02:01:08 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 02:01:08 PM | BATCH_SIZE=32
04/26 02:01:08 PM | DATA_PATH=./data/
04/26 02:01:08 PM | DATASET=fashionmnist
04/26 02:01:08 PM | EPOCHS=10
04/26 02:01:08 PM | GPUS=[0]
04/26 02:01:08 PM | INIT_CHANNELS=16
04/26 02:01:08 PM | LAYERS=8
04/26 02:01:08 PM | NAME=fashionmnist
04/26 02:01:08 PM | PATH=searchs/fashionmnist
04/26 02:01:08 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 02:01:08 PM | PRINT_FREQ=50
04/26 02:01:08 PM | SEED=2
04/26 02:01:08 PM | W_GRAD_CLIP=5.0
04/26 02:01:08 PM | W_LR=0.025
04/26 02:01:08 PM | W_LR_MIN=0.001
04/26 02:01:08 PM | W_MOMENTUM=0.9
04/26 02:01:08 PM | W_WEIGHT_DECAY=0.0003
04/26 02:01:08 PM | WORKERS=2
04/26 02:01:08 PM | 
04/26 02:01:08 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 02:01:14 PM | Train: [ 1/10] Step 000/937 Loss 2.412 Prec@(1,5) (9.4%, 40.6%)
04/26 02:03:09 PM | Train: [ 1/10] Step 050/937 Loss 1.459 Prec@(1,5) (47.5%, 92.8%)
04/26 02:04:36 PM | 
04/26 02:04:36 PM | Parameters:
04/26 02:04:36 PM | ALPHA_LR=0.0003
04/26 02:04:36 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 02:04:36 PM | BATCH_SIZE=32
04/26 02:04:36 PM | DATA_PATH=./data/
04/26 02:04:36 PM | DATASET=fashionmnist
04/26 02:04:36 PM | EPOCHS=10
04/26 02:04:36 PM | GPUS=[0]
04/26 02:04:36 PM | INIT_CHANNELS=16
04/26 02:04:36 PM | LAYERS=4
04/26 02:04:36 PM | NAME=fashionmnist
04/26 02:04:36 PM | PATH=searchs/fashionmnist
04/26 02:04:36 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 02:04:36 PM | PRINT_FREQ=50
04/26 02:04:36 PM | SEED=2
04/26 02:04:36 PM | W_GRAD_CLIP=5.0
04/26 02:04:36 PM | W_LR=0.025
04/26 02:04:36 PM | W_LR_MIN=0.001
04/26 02:04:36 PM | W_MOMENTUM=0.9
04/26 02:04:36 PM | W_WEIGHT_DECAY=0.0003
04/26 02:04:36 PM | WORKERS=2
04/26 02:04:36 PM | 
04/26 02:04:36 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 02:04:40 PM | Train: [ 1/10] Step 000/937 Loss 2.232 Prec@(1,5) (15.6%, 71.9%)
04/26 02:05:39 PM | Train: [ 1/10] Step 050/937 Loss 1.424 Prec@(1,5) (46.9%, 92.9%)
04/26 02:06:40 PM | Train: [ 1/10] Step 100/937 Loss 1.231 Prec@(1,5) (53.5%, 95.5%)
04/26 02:07:40 PM | Train: [ 1/10] Step 150/937 Loss 1.104 Prec@(1,5) (58.1%, 96.6%)
04/26 02:08:39 PM | Train: [ 1/10] Step 200/937 Loss 1.038 Prec@(1,5) (60.6%, 97.0%)
04/26 02:09:37 PM | Train: [ 1/10] Step 250/937 Loss 0.991 Prec@(1,5) (62.7%, 97.4%)
04/26 02:10:37 PM | Train: [ 1/10] Step 300/937 Loss 0.946 Prec@(1,5) (64.4%, 97.7%)
04/26 02:11:36 PM | Train: [ 1/10] Step 350/937 Loss 0.917 Prec@(1,5) (65.5%, 97.9%)
04/26 02:12:35 PM | Train: [ 1/10] Step 400/937 Loss 0.888 Prec@(1,5) (66.6%, 98.0%)
04/26 02:13:34 PM | Train: [ 1/10] Step 450/937 Loss 0.864 Prec@(1,5) (67.5%, 98.2%)
04/26 02:14:33 PM | Train: [ 1/10] Step 500/937 Loss 0.844 Prec@(1,5) (68.4%, 98.3%)
04/26 02:15:32 PM | Train: [ 1/10] Step 550/937 Loss 0.823 Prec@(1,5) (69.1%, 98.4%)
04/26 02:16:31 PM | Train: [ 1/10] Step 600/937 Loss 0.805 Prec@(1,5) (69.8%, 98.5%)
04/26 02:17:30 PM | Train: [ 1/10] Step 650/937 Loss 0.790 Prec@(1,5) (70.3%, 98.6%)
04/26 02:18:29 PM | Train: [ 1/10] Step 700/937 Loss 0.777 Prec@(1,5) (70.8%, 98.6%)
04/26 02:19:28 PM | Train: [ 1/10] Step 750/937 Loss 0.764 Prec@(1,5) (71.3%, 98.7%)
04/26 02:20:27 PM | Train: [ 1/10] Step 800/937 Loss 0.754 Prec@(1,5) (71.7%, 98.7%)
04/26 02:21:27 PM | Train: [ 1/10] Step 850/937 Loss 0.744 Prec@(1,5) (72.1%, 98.8%)
04/26 02:22:25 PM | Train: [ 1/10] Step 900/937 Loss 0.736 Prec@(1,5) (72.4%, 98.8%)
04/26 02:23:09 PM | Train: [ 1/10] Step 937/937 Loss 0.729 Prec@(1,5) (72.7%, 98.8%)
04/26 02:23:09 PM | Train: [ 1/10] Final Prec@1 72.6800%
04/26 02:23:09 PM | Valid: [ 1/10] Step 000/937 Loss 0.630 Prec@(1,5) (71.9%, 100.0%)
04/26 02:23:14 PM | Valid: [ 1/10] Step 050/937 Loss 0.534 Prec@(1,5) (80.1%, 99.3%)
04/26 02:23:20 PM | Valid: [ 1/10] Step 100/937 Loss 0.543 Prec@(1,5) (80.4%, 99.5%)
04/26 02:23:24 PM | Valid: [ 1/10] Step 150/937 Loss 0.552 Prec@(1,5) (80.0%, 99.6%)
04/26 02:23:30 PM | Valid: [ 1/10] Step 200/937 Loss 0.544 Prec@(1,5) (80.2%, 99.7%)
04/26 02:23:35 PM | Valid: [ 1/10] Step 250/937 Loss 0.539 Prec@(1,5) (80.6%, 99.7%)
04/26 02:23:39 PM | Valid: [ 1/10] Step 300/937 Loss 0.540 Prec@(1,5) (80.6%, 99.6%)
04/26 02:23:45 PM | Valid: [ 1/10] Step 350/937 Loss 0.542 Prec@(1,5) (80.5%, 99.6%)
04/26 02:23:50 PM | Valid: [ 1/10] Step 400/937 Loss 0.541 Prec@(1,5) (80.6%, 99.6%)
04/26 02:23:55 PM | Valid: [ 1/10] Step 450/937 Loss 0.538 Prec@(1,5) (80.7%, 99.6%)
04/26 02:24:00 PM | Valid: [ 1/10] Step 500/937 Loss 0.536 Prec@(1,5) (80.8%, 99.6%)
04/26 02:24:05 PM | Valid: [ 1/10] Step 550/937 Loss 0.538 Prec@(1,5) (80.6%, 99.6%)
04/26 02:24:10 PM | Valid: [ 1/10] Step 600/937 Loss 0.541 Prec@(1,5) (80.5%, 99.6%)
04/26 02:24:15 PM | Valid: [ 1/10] Step 650/937 Loss 0.540 Prec@(1,5) (80.6%, 99.6%)
04/26 02:24:20 PM | Valid: [ 1/10] Step 700/937 Loss 0.544 Prec@(1,5) (80.4%, 99.6%)
04/26 02:24:25 PM | Valid: [ 1/10] Step 750/937 Loss 0.542 Prec@(1,5) (80.5%, 99.6%)
04/26 02:24:30 PM | Valid: [ 1/10] Step 800/937 Loss 0.546 Prec@(1,5) (80.3%, 99.6%)
04/26 02:24:35 PM | Valid: [ 1/10] Step 850/937 Loss 0.545 Prec@(1,5) (80.4%, 99.6%)
04/26 02:24:40 PM | Valid: [ 1/10] Step 900/937 Loss 0.545 Prec@(1,5) (80.3%, 99.6%)
04/26 02:24:44 PM | Valid: [ 1/10] Step 937/937 Loss 0.547 Prec@(1,5) (80.3%, 99.6%)
04/26 02:24:44 PM | Valid: [ 1/10] Final Prec@1 80.3100%
04/26 02:24:44 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('max_pool_3x3', 1)], [('dil_conv_5x5', 1), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)], [('dil_conv_5x5', 2), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1287, 0.1187, 0.1200, 0.1272, 0.1257, 0.1258, 0.1268, 0.1271],
        [0.1302, 0.1189, 0.1211, 0.1261, 0.1281, 0.1221, 0.1265, 0.1271]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1264, 0.1184, 0.1197, 0.1287, 0.1278, 0.1257, 0.1253, 0.1280],
        [0.1295, 0.1189, 0.1203, 0.1253, 0.1250, 0.1255, 0.1274, 0.1281],
        [0.1275, 0.1176, 0.1186, 0.1260, 0.1290, 0.1247, 0.1309, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1259, 0.1180, 0.1189, 0.1271, 0.1261, 0.1270, 0.1287, 0.1283],
        [0.1260, 0.1171, 0.1179, 0.1274, 0.1273, 0.1252, 0.1307, 0.1284],
        [0.1263, 0.1176, 0.1188, 0.1243, 0.1269, 0.1276, 0.1286, 0.1299],
        [0.1260, 0.1186, 0.1192, 0.1269, 0.1268, 0.1288, 0.1247, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1243, 0.1185, 0.1195, 0.1266, 0.1290, 0.1266, 0.1262, 0.1293],
        [0.1253, 0.1177, 0.1179, 0.1263, 0.1284, 0.1273, 0.1278, 0.1293],
        [0.1240, 0.1175, 0.1176, 0.1257, 0.1290, 0.1271, 0.1298, 0.1291],
        [0.1237, 0.1173, 0.1170, 0.1283, 0.1282, 0.1288, 0.1290, 0.1278],
        [0.1227, 0.1174, 0.1178, 0.1287, 0.1292, 0.1271, 0.1285, 0.1286]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1275, 0.1223, 0.1232, 0.1237, 0.1256, 0.1244, 0.1247, 0.1286],
        [0.1257, 0.1206, 0.1232, 0.1248, 0.1300, 0.1244, 0.1285, 0.1227]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1259, 0.1213, 0.1239, 0.1257, 0.1283, 0.1248, 0.1215, 0.1285],
        [0.1256, 0.1211, 0.1249, 0.1248, 0.1287, 0.1262, 0.1243, 0.1244],
        [0.1298, 0.1139, 0.1189, 0.1268, 0.1286, 0.1283, 0.1304, 0.1233]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1241, 0.1204, 0.1245, 0.1250, 0.1261, 0.1248, 0.1260, 0.1290],
        [0.1238, 0.1194, 0.1257, 0.1235, 0.1297, 0.1260, 0.1255, 0.1264],
        [0.1293, 0.1127, 0.1200, 0.1243, 0.1297, 0.1269, 0.1319, 0.1251],
        [0.1271, 0.1139, 0.1200, 0.1297, 0.1323, 0.1275, 0.1264, 0.1231]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1240, 0.1213, 0.1238, 0.1251, 0.1266, 0.1235, 0.1256, 0.1301],
        [0.1239, 0.1201, 0.1260, 0.1232, 0.1278, 0.1248, 0.1282, 0.1261],
        [0.1287, 0.1138, 0.1170, 0.1285, 0.1288, 0.1277, 0.1307, 0.1247],
        [0.1276, 0.1148, 0.1183, 0.1307, 0.1289, 0.1272, 0.1296, 0.1229],
        [0.1272, 0.1167, 0.1188, 0.1281, 0.1302, 0.1255, 0.1302, 0.1232]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 02:24:46 PM | Train: [ 2/10] Step 000/937 Loss 0.557 Prec@(1,5) (75.0%, 100.0%)
04/26 02:25:47 PM | Train: [ 2/10] Step 050/937 Loss 0.518 Prec@(1,5) (79.7%, 99.8%)
04/26 02:26:48 PM | Train: [ 2/10] Step 100/937 Loss 0.499 Prec@(1,5) (80.7%, 99.7%)
04/26 02:27:49 PM | Train: [ 2/10] Step 150/937 Loss 0.522 Prec@(1,5) (80.0%, 99.7%)
04/26 02:28:50 PM | Train: [ 2/10] Step 200/937 Loss 0.522 Prec@(1,5) (79.9%, 99.6%)
04/26 02:29:52 PM | Train: [ 2/10] Step 250/937 Loss 0.518 Prec@(1,5) (80.3%, 99.6%)
04/26 02:30:55 PM | Train: [ 2/10] Step 300/937 Loss 0.518 Prec@(1,5) (80.3%, 99.6%)
04/26 02:31:59 PM | Train: [ 2/10] Step 350/937 Loss 0.512 Prec@(1,5) (80.5%, 99.6%)
04/26 02:33:02 PM | Train: [ 2/10] Step 400/937 Loss 0.509 Prec@(1,5) (80.7%, 99.6%)
04/26 02:34:06 PM | Train: [ 2/10] Step 450/937 Loss 0.504 Prec@(1,5) (81.0%, 99.6%)
04/26 02:35:10 PM | Train: [ 2/10] Step 500/937 Loss 0.502 Prec@(1,5) (81.1%, 99.6%)
04/26 02:36:14 PM | Train: [ 2/10] Step 550/937 Loss 0.499 Prec@(1,5) (81.3%, 99.6%)
04/26 02:37:18 PM | Train: [ 2/10] Step 600/937 Loss 0.497 Prec@(1,5) (81.3%, 99.6%)
04/26 02:38:21 PM | Train: [ 2/10] Step 650/937 Loss 0.495 Prec@(1,5) (81.5%, 99.6%)
04/26 02:39:25 PM | Train: [ 2/10] Step 700/937 Loss 0.496 Prec@(1,5) (81.5%, 99.6%)
04/26 02:40:29 PM | Train: [ 2/10] Step 750/937 Loss 0.494 Prec@(1,5) (81.6%, 99.6%)
04/26 02:41:32 PM | Train: [ 2/10] Step 800/937 Loss 0.493 Prec@(1,5) (81.7%, 99.6%)
04/26 02:42:35 PM | Train: [ 2/10] Step 850/937 Loss 0.491 Prec@(1,5) (81.7%, 99.6%)
04/26 02:43:36 PM | Train: [ 2/10] Step 900/937 Loss 0.490 Prec@(1,5) (81.8%, 99.6%)
04/26 02:44:22 PM | Train: [ 2/10] Step 937/937 Loss 0.489 Prec@(1,5) (81.9%, 99.6%)
04/26 02:44:22 PM | Train: [ 2/10] Final Prec@1 81.8600%
04/26 02:44:22 PM | Valid: [ 2/10] Step 000/937 Loss 0.599 Prec@(1,5) (75.0%, 100.0%)
04/26 02:44:27 PM | Valid: [ 2/10] Step 050/937 Loss 0.416 Prec@(1,5) (84.1%, 99.9%)
04/26 02:44:33 PM | Valid: [ 2/10] Step 100/937 Loss 0.433 Prec@(1,5) (84.3%, 99.8%)
04/26 02:44:37 PM | Valid: [ 2/10] Step 150/937 Loss 0.449 Prec@(1,5) (83.9%, 99.7%)
04/26 02:44:43 PM | Valid: [ 2/10] Step 200/937 Loss 0.448 Prec@(1,5) (83.6%, 99.7%)
04/26 02:44:48 PM | Valid: [ 2/10] Step 250/937 Loss 0.449 Prec@(1,5) (83.6%, 99.7%)
04/26 02:44:53 PM | Valid: [ 2/10] Step 300/937 Loss 0.450 Prec@(1,5) (83.5%, 99.7%)
04/26 02:44:58 PM | Valid: [ 2/10] Step 350/937 Loss 0.449 Prec@(1,5) (83.4%, 99.7%)
04/26 02:45:03 PM | Valid: [ 2/10] Step 400/937 Loss 0.444 Prec@(1,5) (83.7%, 99.7%)
04/26 02:45:09 PM | Valid: [ 2/10] Step 450/937 Loss 0.443 Prec@(1,5) (83.8%, 99.7%)
04/26 02:45:14 PM | Valid: [ 2/10] Step 500/937 Loss 0.443 Prec@(1,5) (83.9%, 99.7%)
04/26 02:45:19 PM | Valid: [ 2/10] Step 550/937 Loss 0.442 Prec@(1,5) (83.9%, 99.7%)
04/26 02:45:25 PM | Valid: [ 2/10] Step 600/937 Loss 0.446 Prec@(1,5) (83.8%, 99.7%)
04/26 02:45:30 PM | Valid: [ 2/10] Step 650/937 Loss 0.446 Prec@(1,5) (83.7%, 99.7%)
04/26 02:45:35 PM | Valid: [ 2/10] Step 700/937 Loss 0.447 Prec@(1,5) (83.7%, 99.7%)
04/26 02:45:40 PM | Valid: [ 2/10] Step 750/937 Loss 0.446 Prec@(1,5) (83.7%, 99.7%)
04/26 02:45:46 PM | Valid: [ 2/10] Step 800/937 Loss 0.447 Prec@(1,5) (83.7%, 99.7%)
04/26 02:45:50 PM | Valid: [ 2/10] Step 850/937 Loss 0.451 Prec@(1,5) (83.6%, 99.7%)
04/26 02:45:55 PM | Valid: [ 2/10] Step 900/937 Loss 0.451 Prec@(1,5) (83.6%, 99.7%)
04/26 02:46:00 PM | Valid: [ 2/10] Step 937/937 Loss 0.451 Prec@(1,5) (83.5%, 99.7%)
04/26 02:46:00 PM | Valid: [ 2/10] Final Prec@1 83.5467%
04/26 02:46:00 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 0)], [('dil_conv_3x3', 3), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 2), ('sep_conv_5x5', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 4), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1284, 0.1128, 0.1145, 0.1346, 0.1290, 0.1252, 0.1269, 0.1286],
        [0.1337, 0.1154, 0.1194, 0.1271, 0.1293, 0.1198, 0.1278, 0.1275]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1264, 0.1136, 0.1153, 0.1295, 0.1303, 0.1268, 0.1283, 0.1299],
        [0.1329, 0.1156, 0.1186, 0.1250, 0.1247, 0.1237, 0.1293, 0.1302],
        [0.1295, 0.1120, 0.1143, 0.1276, 0.1341, 0.1243, 0.1338, 0.1245]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1268, 0.1138, 0.1146, 0.1287, 0.1288, 0.1264, 0.1306, 0.1303],
        [0.1285, 0.1131, 0.1147, 0.1283, 0.1265, 0.1231, 0.1352, 0.1307],
        [0.1293, 0.1131, 0.1151, 0.1257, 0.1271, 0.1284, 0.1292, 0.1321],
        [0.1296, 0.1160, 0.1166, 0.1296, 0.1273, 0.1281, 0.1230, 0.1298]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1259, 0.1152, 0.1165, 0.1261, 0.1290, 0.1259, 0.1281, 0.1333],
        [0.1280, 0.1144, 0.1150, 0.1273, 0.1283, 0.1297, 0.1272, 0.1302],
        [0.1262, 0.1140, 0.1157, 0.1254, 0.1282, 0.1286, 0.1305, 0.1313],
        [0.1257, 0.1137, 0.1131, 0.1279, 0.1302, 0.1314, 0.1311, 0.1270],
        [0.1242, 0.1150, 0.1154, 0.1301, 0.1301, 0.1246, 0.1307, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1275, 0.1216, 0.1232, 0.1230, 0.1272, 0.1235, 0.1227, 0.1313],
        [0.1259, 0.1185, 0.1213, 0.1254, 0.1360, 0.1229, 0.1293, 0.1207]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1256, 0.1210, 0.1225, 0.1256, 0.1268, 0.1204, 0.1246, 0.1335],
        [0.1258, 0.1190, 0.1261, 0.1265, 0.1309, 0.1244, 0.1228, 0.1245],
        [0.1308, 0.1079, 0.1154, 0.1295, 0.1348, 0.1272, 0.1334, 0.1211]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1240, 0.1199, 0.1239, 0.1239, 0.1266, 0.1231, 0.1248, 0.1336],
        [0.1235, 0.1172, 0.1248, 0.1238, 0.1354, 0.1237, 0.1258, 0.1259],
        [0.1314, 0.1075, 0.1166, 0.1243, 0.1327, 0.1282, 0.1368, 0.1226],
        [0.1286, 0.1085, 0.1173, 0.1339, 0.1357, 0.1300, 0.1251, 0.1208]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1231, 0.1213, 0.1228, 0.1242, 0.1265, 0.1222, 0.1252, 0.1347],
        [0.1240, 0.1196, 0.1259, 0.1231, 0.1290, 0.1217, 0.1284, 0.1282],
        [0.1300, 0.1089, 0.1131, 0.1289, 0.1323, 0.1302, 0.1338, 0.1229],
        [0.1293, 0.1098, 0.1136, 0.1331, 0.1320, 0.1296, 0.1320, 0.1206],
        [0.1284, 0.1116, 0.1159, 0.1310, 0.1347, 0.1263, 0.1320, 0.1202]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 02:46:02 PM | Train: [ 3/10] Step 000/937 Loss 0.460 Prec@(1,5) (87.5%, 100.0%)
04/26 02:47:05 PM | Train: [ 3/10] Step 050/937 Loss 0.424 Prec@(1,5) (85.1%, 99.6%)
04/26 02:48:08 PM | Train: [ 3/10] Step 100/937 Loss 0.426 Prec@(1,5) (84.4%, 99.6%)
04/26 02:49:10 PM | Train: [ 3/10] Step 150/937 Loss 0.433 Prec@(1,5) (84.1%, 99.6%)
04/26 02:50:12 PM | Train: [ 3/10] Step 200/937 Loss 0.426 Prec@(1,5) (84.3%, 99.7%)
04/26 02:51:15 PM | Train: [ 3/10] Step 250/937 Loss 0.427 Prec@(1,5) (84.3%, 99.7%)
04/26 02:52:17 PM | Train: [ 3/10] Step 300/937 Loss 0.427 Prec@(1,5) (84.2%, 99.7%)
04/26 02:53:19 PM | Train: [ 3/10] Step 350/937 Loss 0.432 Prec@(1,5) (84.2%, 99.7%)
04/26 02:54:23 PM | Train: [ 3/10] Step 400/937 Loss 0.431 Prec@(1,5) (84.4%, 99.7%)
04/26 02:55:30 PM | Train: [ 3/10] Step 450/937 Loss 0.428 Prec@(1,5) (84.4%, 99.7%)
04/26 02:56:34 PM | Train: [ 3/10] Step 500/937 Loss 0.430 Prec@(1,5) (84.3%, 99.7%)
04/26 02:57:40 PM | Train: [ 3/10] Step 550/937 Loss 0.431 Prec@(1,5) (84.2%, 99.7%)
04/26 02:58:54 PM | 
04/26 02:58:54 PM | Parameters:
04/26 02:58:54 PM | ALPHA_LR=0.0003
04/26 02:58:54 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 02:58:54 PM | BATCH_SIZE=32
04/26 02:58:54 PM | DATA_PATH=./data/
04/26 02:58:54 PM | DATASET=fashionmnist
04/26 02:58:54 PM | EPOCHS=3
04/26 02:58:54 PM | GPUS=[0]
04/26 02:58:54 PM | INIT_CHANNELS=16
04/26 02:58:54 PM | LAYERS=4
04/26 02:58:54 PM | NAME=fashionmnist
04/26 02:58:54 PM | PATH=searchs/fashionmnist
04/26 02:58:54 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 02:58:54 PM | PRINT_FREQ=50
04/26 02:58:54 PM | SEED=2
04/26 02:58:54 PM | W_GRAD_CLIP=5.0
04/26 02:58:54 PM | W_LR=0.025
04/26 02:58:54 PM | W_LR_MIN=0.001
04/26 02:58:54 PM | W_MOMENTUM=0.9
04/26 02:58:54 PM | W_WEIGHT_DECAY=0.0003
04/26 02:58:54 PM | WORKERS=4
04/26 02:58:54 PM | 
04/26 02:58:54 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 02:58:57 PM | Train: [ 1/3] Step 000/937 Loss 2.232 Prec@(1,5) (15.6%, 71.9%)
04/26 02:59:56 PM | 
04/26 02:59:56 PM | Parameters:
04/26 02:59:56 PM | ALPHA_LR=0.0003
04/26 02:59:56 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 02:59:56 PM | BATCH_SIZE=32
04/26 02:59:56 PM | DATA_PATH=./data/
04/26 02:59:56 PM | DATASET=fashionmnist
04/26 02:59:56 PM | EPOCHS=3
04/26 02:59:56 PM | GPUS=[0]
04/26 02:59:56 PM | INIT_CHANNELS=16
04/26 02:59:56 PM | LAYERS=4
04/26 02:59:56 PM | NAME=fashionmnist
04/26 02:59:56 PM | PATH=searchs/fashionmnist
04/26 02:59:56 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 02:59:56 PM | PRINT_FREQ=50
04/26 02:59:56 PM | SEED=2
04/26 02:59:56 PM | W_GRAD_CLIP=5.0
04/26 02:59:56 PM | W_LR=0.025
04/26 02:59:56 PM | W_LR_MIN=0.001
04/26 02:59:56 PM | W_MOMENTUM=0.9
04/26 02:59:56 PM | W_WEIGHT_DECAY=0.0003
04/26 02:59:56 PM | WORKERS=4
04/26 02:59:56 PM | 
04/26 02:59:56 PM | Logger is set - training start
04/26 03:04:41 PM | 
04/26 03:04:41 PM | Parameters:
04/26 03:04:41 PM | ALPHA_LR=0.0003
04/26 03:04:41 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 03:04:41 PM | BATCH_SIZE=32
04/26 03:04:41 PM | DATA_PATH=./data/
04/26 03:04:41 PM | DATASET=fashionmnist
04/26 03:04:41 PM | EPOCHS=3
04/26 03:04:41 PM | GPUS=[0]
04/26 03:04:41 PM | INIT_CHANNELS=16
04/26 03:04:41 PM | LAYERS=4
04/26 03:04:41 PM | NAME=fashionmnist
04/26 03:04:41 PM | PATH=searchs/fashionmnist
04/26 03:04:41 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 03:04:41 PM | PRINT_FREQ=50
04/26 03:04:41 PM | SEED=2
04/26 03:04:41 PM | W_GRAD_CLIP=5.0
04/26 03:04:41 PM | W_LR=0.025
04/26 03:04:41 PM | W_LR_MIN=0.001
04/26 03:04:41 PM | W_MOMENTUM=0.9
04/26 03:04:41 PM | W_WEIGHT_DECAY=0.0003
04/26 03:04:41 PM | WORKERS=4
04/26 03:04:41 PM | 
04/26 03:04:41 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1110, 0.1112, 0.1110, 0.1112, 0.1111, 0.1111, 0.1112, 0.1112, 0.1110],
        [0.1111, 0.1111, 0.1114, 0.1111, 0.1112, 0.1111, 0.1112, 0.1110, 0.1109]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1110, 0.1113, 0.1111, 0.1112, 0.1112, 0.1110, 0.1112, 0.1110, 0.1111],
        [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1112, 0.1112, 0.1110],
        [0.1112, 0.1110, 0.1111, 0.1111, 0.1111, 0.1111, 0.1112, 0.1111, 0.1112]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1110, 0.1112, 0.1111, 0.1112, 0.1112, 0.1109, 0.1111, 0.1113, 0.1111],
        [0.1111, 0.1111, 0.1109, 0.1113, 0.1111, 0.1111, 0.1112, 0.1111, 0.1110],
        [0.1112, 0.1110, 0.1111, 0.1111, 0.1112, 0.1112, 0.1111, 0.1112, 0.1110],
        [0.1111, 0.1109, 0.1110, 0.1113, 0.1112, 0.1112, 0.1111, 0.1111, 0.1110]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1113, 0.1113, 0.1111, 0.1111, 0.1110, 0.1110, 0.1111, 0.1109, 0.1113],
        [0.1113, 0.1111, 0.1111, 0.1110, 0.1110, 0.1112, 0.1111, 0.1111, 0.1112],
        [0.1111, 0.1112, 0.1110, 0.1110, 0.1112, 0.1112, 0.1111, 0.1112, 0.1110],
        [0.1112, 0.1110, 0.1111, 0.1111, 0.1110, 0.1112, 0.1111, 0.1111, 0.1111],
        [0.1110, 0.1112, 0.1112, 0.1108, 0.1114, 0.1111, 0.1112, 0.1113, 0.1109]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1111, 0.1111, 0.1111, 0.1113, 0.1110, 0.1112, 0.1112, 0.1109, 0.1110],
        [0.1114, 0.1109, 0.1112, 0.1110, 0.1111, 0.1111, 0.1111, 0.1110, 0.1112]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1112, 0.1111, 0.1112, 0.1110, 0.1110],
        [0.1110, 0.1110, 0.1114, 0.1112, 0.1111, 0.1111, 0.1109, 0.1109, 0.1112],
        [0.1109, 0.1112, 0.1110, 0.1111, 0.1112, 0.1110, 0.1112, 0.1112, 0.1111]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1112, 0.1110, 0.1110, 0.1111, 0.1111, 0.1112, 0.1110, 0.1112, 0.1112],
        [0.1111, 0.1111, 0.1112, 0.1111, 0.1111, 0.1111, 0.1112, 0.1109, 0.1110],
        [0.1110, 0.1110, 0.1111, 0.1111, 0.1109, 0.1113, 0.1113, 0.1112, 0.1110],
        [0.1112, 0.1112, 0.1111, 0.1110, 0.1112, 0.1113, 0.1110, 0.1111, 0.1111]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1110, 0.1110, 0.1112, 0.1112, 0.1109, 0.1110, 0.1111, 0.1112, 0.1113],
        [0.1111, 0.1111, 0.1112, 0.1111, 0.1109, 0.1111, 0.1111, 0.1112, 0.1112],
        [0.1111, 0.1113, 0.1110, 0.1111, 0.1111, 0.1111, 0.1110, 0.1109, 0.1112],
        [0.1112, 0.1111, 0.1112, 0.1109, 0.1111, 0.1111, 0.1111, 0.1111, 0.1110],
        [0.1111, 0.1110, 0.1110, 0.1113, 0.1110, 0.1112, 0.1112, 0.1111, 0.1110]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 03:04:46 PM | Train: [ 1/3] Step 000/937 Loss 2.407 Prec@(1,5) (12.5%, 50.0%)
04/26 03:06:02 PM | Train: [ 1/3] Step 050/937 Loss 1.442 Prec@(1,5) (48.7%, 91.4%)
04/26 03:07:19 PM | Train: [ 1/3] Step 100/937 Loss 1.249 Prec@(1,5) (53.7%, 94.6%)
04/26 03:08:52 PM | 
04/26 03:08:52 PM | Parameters:
04/26 03:08:52 PM | ALPHA_LR=0.0003
04/26 03:08:52 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 03:08:52 PM | BATCH_SIZE=32
04/26 03:08:52 PM | DATA_PATH=./data/
04/26 03:08:52 PM | DATASET=fashionmnist
04/26 03:08:52 PM | EPOCHS=3
04/26 03:08:52 PM | GPUS=[0]
04/26 03:08:52 PM | INIT_CHANNELS=16
04/26 03:08:52 PM | LAYERS=4
04/26 03:08:52 PM | NAME=fashionmnist
04/26 03:08:52 PM | PATH=searchs/fashionmnist
04/26 03:08:52 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 03:08:52 PM | PRINT_FREQ=50
04/26 03:08:52 PM | SEED=2
04/26 03:08:52 PM | W_GRAD_CLIP=5.0
04/26 03:08:52 PM | W_LR=0.025
04/26 03:08:52 PM | W_LR_MIN=0.001
04/26 03:08:52 PM | W_MOMENTUM=0.9
04/26 03:08:52 PM | W_WEIGHT_DECAY=0.0003
04/26 03:08:52 PM | WORKERS=4
04/26 03:08:52 PM | 
04/26 03:08:52 PM | Logger is set - training start
04/26 03:09:48 PM | 
04/26 03:09:48 PM | Parameters:
04/26 03:09:48 PM | ALPHA_LR=0.0003
04/26 03:09:48 PM | ALPHA_WEIGHT_DECAY=0.001
04/26 03:09:48 PM | BATCH_SIZE=32
04/26 03:09:48 PM | DATA_PATH=./data/
04/26 03:09:48 PM | DATASET=fashionmnist
04/26 03:09:48 PM | EPOCHS=3
04/26 03:09:48 PM | GPUS=[0]
04/26 03:09:48 PM | INIT_CHANNELS=16
04/26 03:09:48 PM | LAYERS=4
04/26 03:09:48 PM | NAME=fashionmnist
04/26 03:09:48 PM | PATH=searchs/fashionmnist
04/26 03:09:48 PM | PLOT_PATH=searchs/fashionmnist/plots
04/26 03:09:48 PM | PRINT_FREQ=50
04/26 03:09:48 PM | SEED=2
04/26 03:09:48 PM | W_GRAD_CLIP=5.0
04/26 03:09:48 PM | W_LR=0.025
04/26 03:09:48 PM | W_LR_MIN=0.001
04/26 03:09:48 PM | W_MOMENTUM=0.9
04/26 03:09:48 PM | W_WEIGHT_DECAY=0.0003
04/26 03:09:48 PM | WORKERS=4
04/26 03:09:48 PM | 
04/26 03:09:48 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.0999, 0.1001, 0.0999, 0.0999, 0.1000, 0.1000, 0.1001, 0.1001, 0.0999,
         0.1001],
        [0.0999, 0.1000, 0.1000, 0.1001, 0.1000, 0.1002, 0.0999, 0.0999, 0.1000,
         0.1001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1001, 0.0999, 0.1001, 0.0999, 0.1000, 0.1000, 0.0999, 0.1000, 0.1000,
         0.1001],
        [0.1000, 0.0999, 0.0999, 0.1001, 0.1001, 0.1001, 0.1000, 0.0999, 0.1001,
         0.0999],
        [0.0999, 0.1001, 0.0999, 0.1001, 0.1001, 0.1000, 0.1001, 0.1000, 0.0999,
         0.0999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1001, 0.0999, 0.1001, 0.0999, 0.1001, 0.1000, 0.1001, 0.1000, 0.0999,
         0.1001],
        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0999, 0.1001, 0.1000, 0.0999,
         0.1001],
        [0.1000, 0.1000, 0.1000, 0.1000, 0.0999, 0.1000, 0.1001, 0.1000, 0.0999,
         0.1000],
        [0.0999, 0.1001, 0.1000, 0.1002, 0.1001, 0.0999, 0.0999, 0.1000, 0.0999,
         0.1000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1000, 0.1001, 0.0999, 0.0999, 0.1001, 0.1000, 0.1000, 0.1000, 0.0999,
         0.1001],
        [0.0999, 0.1001, 0.0999, 0.1001, 0.1001, 0.1000, 0.0999, 0.1000, 0.1002,
         0.1000],
        [0.0998, 0.1002, 0.1001, 0.1001, 0.1001, 0.1000, 0.0997, 0.1000, 0.1001,
         0.1000],
        [0.1000, 0.1001, 0.1000, 0.1000, 0.1001, 0.0999, 0.1001, 0.1001, 0.1000,
         0.0999],
        [0.1001, 0.1001, 0.1000, 0.0999, 0.0999, 0.1002, 0.0999, 0.1000, 0.0999,
         0.1000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.0999, 0.1000, 0.1000, 0.1000, 0.1001, 0.0998, 0.1000, 0.1003, 0.0999,
         0.1000],
        [0.1001, 0.1002, 0.0999, 0.1000, 0.0999, 0.1000, 0.1001, 0.0999, 0.1000,
         0.0999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0999, 0.1001, 0.1002, 0.1001, 0.1000, 0.0999, 0.0999, 0.1000, 0.1001,
         0.0998],
        [0.1000, 0.1000, 0.1000, 0.0998, 0.1002, 0.0999, 0.1001, 0.0999, 0.1000,
         0.1000],
        [0.0999, 0.1000, 0.1003, 0.1000, 0.1000, 0.0999, 0.1000, 0.0999, 0.1000,
         0.1000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1001, 0.1001, 0.0998, 0.0999, 0.1000, 0.1000, 0.1001, 0.0999, 0.1000,
         0.1001],
        [0.1000, 0.0999, 0.0999, 0.1000, 0.1001, 0.1000, 0.1001, 0.1000, 0.1000,
         0.1000],
        [0.1000, 0.1001, 0.1001, 0.1000, 0.1000, 0.1000, 0.1000, 0.0999, 0.1000,
         0.0999],
        [0.1001, 0.1001, 0.0999, 0.1000, 0.0999, 0.0999, 0.1001, 0.1000, 0.1001,
         0.1001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1000, 0.0998, 0.1002, 0.0999, 0.1000, 0.1000, 0.1000, 0.1000, 0.1001,
         0.0999],
        [0.1001, 0.1000, 0.1000, 0.0999, 0.0999, 0.1002, 0.0998, 0.1000, 0.1002,
         0.1000],
        [0.1000, 0.1000, 0.1000, 0.0998, 0.0998, 0.1001, 0.1001, 0.1000, 0.1001,
         0.1001],
        [0.0999, 0.1000, 0.1001, 0.1000, 0.1000, 0.1000, 0.0998, 0.1001, 0.1002,
         0.1001],
        [0.1001, 0.1002, 0.1000, 0.0999, 0.0999, 0.1000, 0.1000, 0.0997, 0.1002,
         0.1001]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 03:09:52 PM | Train: [ 1/3] Step 000/937 Loss 2.376 Prec@(1,5) (18.8%, 37.5%)
04/26 03:11:20 PM | Train: [ 1/3] Step 050/937 Loss 1.431 Prec@(1,5) (47.8%, 91.7%)
04/26 03:12:48 PM | Train: [ 1/3] Step 100/937 Loss 1.244 Prec@(1,5) (55.0%, 94.7%)
04/26 03:14:17 PM | Train: [ 1/3] Step 150/937 Loss 1.116 Prec@(1,5) (59.6%, 95.9%)
04/26 03:15:45 PM | Train: [ 1/3] Step 200/937 Loss 1.056 Prec@(1,5) (61.6%, 96.7%)
04/26 03:17:12 PM | Train: [ 1/3] Step 250/937 Loss 1.004 Prec@(1,5) (63.2%, 97.0%)
04/26 03:18:40 PM | Train: [ 1/3] Step 300/937 Loss 0.968 Prec@(1,5) (64.4%, 97.3%)
04/26 03:20:10 PM | Train: [ 1/3] Step 350/937 Loss 0.931 Prec@(1,5) (65.6%, 97.6%)
04/26 03:21:38 PM | Train: [ 1/3] Step 400/937 Loss 0.902 Prec@(1,5) (66.7%, 97.8%)
04/26 03:23:08 PM | Train: [ 1/3] Step 450/937 Loss 0.876 Prec@(1,5) (67.6%, 97.9%)
04/26 03:24:36 PM | Train: [ 1/3] Step 500/937 Loss 0.856 Prec@(1,5) (68.3%, 98.1%)
04/26 03:26:07 PM | Train: [ 1/3] Step 550/937 Loss 0.842 Prec@(1,5) (68.7%, 98.2%)
04/26 03:27:37 PM | Train: [ 1/3] Step 600/937 Loss 0.825 Prec@(1,5) (69.3%, 98.3%)
04/26 03:29:06 PM | Train: [ 1/3] Step 650/937 Loss 0.811 Prec@(1,5) (69.8%, 98.3%)
04/26 03:30:35 PM | Train: [ 1/3] Step 700/937 Loss 0.803 Prec@(1,5) (70.1%, 98.4%)
04/26 03:32:05 PM | Train: [ 1/3] Step 750/937 Loss 0.790 Prec@(1,5) (70.5%, 98.5%)
04/26 03:33:33 PM | Train: [ 1/3] Step 800/937 Loss 0.777 Prec@(1,5) (71.1%, 98.5%)
04/26 03:35:02 PM | Train: [ 1/3] Step 850/937 Loss 0.763 Prec@(1,5) (71.5%, 98.6%)
04/26 03:36:32 PM | Train: [ 1/3] Step 900/937 Loss 0.750 Prec@(1,5) (72.0%, 98.7%)
04/26 03:37:38 PM | Train: [ 1/3] Step 937/937 Loss 0.744 Prec@(1,5) (72.2%, 98.7%)
04/26 03:37:38 PM | Train: [ 1/3] Final Prec@1 72.2167%
04/26 03:37:39 PM | Valid: [ 1/3] Step 000/937 Loss 0.358 Prec@(1,5) (87.5%, 100.0%)
04/26 03:37:47 PM | Valid: [ 1/3] Step 050/937 Loss 0.555 Prec@(1,5) (80.4%, 99.2%)
04/26 03:37:54 PM | Valid: [ 1/3] Step 100/937 Loss 0.554 Prec@(1,5) (79.6%, 99.2%)
04/26 03:38:01 PM | Valid: [ 1/3] Step 150/937 Loss 0.542 Prec@(1,5) (79.9%, 99.3%)
04/26 03:38:08 PM | Valid: [ 1/3] Step 200/937 Loss 0.546 Prec@(1,5) (80.3%, 99.3%)
04/26 03:38:16 PM | Valid: [ 1/3] Step 250/937 Loss 0.546 Prec@(1,5) (80.0%, 99.3%)
04/26 03:38:23 PM | Valid: [ 1/3] Step 300/937 Loss 0.550 Prec@(1,5) (79.8%, 99.4%)
04/26 03:38:30 PM | Valid: [ 1/3] Step 350/937 Loss 0.551 Prec@(1,5) (79.6%, 99.4%)
04/26 03:38:38 PM | Valid: [ 1/3] Step 400/937 Loss 0.556 Prec@(1,5) (79.6%, 99.3%)
04/26 03:38:45 PM | Valid: [ 1/3] Step 450/937 Loss 0.555 Prec@(1,5) (79.7%, 99.3%)
04/26 03:38:52 PM | Valid: [ 1/3] Step 500/937 Loss 0.553 Prec@(1,5) (79.9%, 99.3%)
04/26 03:39:00 PM | Valid: [ 1/3] Step 550/937 Loss 0.554 Prec@(1,5) (79.9%, 99.3%)
04/26 03:39:07 PM | Valid: [ 1/3] Step 600/937 Loss 0.554 Prec@(1,5) (79.9%, 99.3%)
04/26 03:39:14 PM | Valid: [ 1/3] Step 650/937 Loss 0.552 Prec@(1,5) (80.0%, 99.3%)
04/26 03:39:22 PM | Valid: [ 1/3] Step 700/937 Loss 0.551 Prec@(1,5) (80.0%, 99.3%)
04/26 03:39:29 PM | Valid: [ 1/3] Step 750/937 Loss 0.552 Prec@(1,5) (80.0%, 99.3%)
04/26 03:39:36 PM | Valid: [ 1/3] Step 800/937 Loss 0.554 Prec@(1,5) (79.9%, 99.3%)
04/26 03:39:44 PM | Valid: [ 1/3] Step 850/937 Loss 0.554 Prec@(1,5) (79.8%, 99.3%)
04/26 03:39:51 PM | Valid: [ 1/3] Step 900/937 Loss 0.553 Prec@(1,5) (79.8%, 99.3%)
04/26 03:39:56 PM | Valid: [ 1/3] Step 937/937 Loss 0.553 Prec@(1,5) (79.8%, 99.3%)
04/26 03:39:56 PM | Valid: [ 1/3] Final Prec@1 79.8433%
04/26 03:39:56 PM | genotype = Genotype(normal=[[('sep_conv_5x5', 0), ('sep_conv_5x5', 1)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 0)], [('sep_conv_5x5', 0), ('sep_conv_3x3', 3)], [('dil_conv_5x5', 0), ('sep_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)], [('sep_conv_5x5', 2), ('max_pool_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0983, 0.0937, 0.0930, 0.1047, 0.1060, 0.1027, 0.1038, 0.0976, 0.0943,
         0.1059],
        [0.1028, 0.0958, 0.0960, 0.1032, 0.1032, 0.1002, 0.1017, 0.0973, 0.0986,
         0.1013]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0979, 0.0947, 0.0948, 0.1015, 0.1044, 0.1028, 0.1046, 0.0989, 0.0950,
         0.1055],
        [0.1024, 0.0955, 0.0950, 0.1007, 0.1046, 0.1013, 0.1022, 0.0976, 0.0977,
         0.1029],
        [0.1001, 0.0935, 0.0926, 0.1008, 0.1034, 0.1026, 0.1071, 0.0995, 0.0974,
         0.1031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0969, 0.0936, 0.0941, 0.1020, 0.1058, 0.1032, 0.1042, 0.0965, 0.0970,
         0.1067],
        [0.1004, 0.0956, 0.0952, 0.1030, 0.1045, 0.1027, 0.1027, 0.0967, 0.0956,
         0.1036],
        [0.0983, 0.0929, 0.0930, 0.1050, 0.1055, 0.1042, 0.1028, 0.0968, 0.0965,
         0.1050],
        [0.0975, 0.0930, 0.0925, 0.1059, 0.1044, 0.1050, 0.1041, 0.0967, 0.0978,
         0.1031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0970, 0.0934, 0.0933, 0.1040, 0.1049, 0.1046, 0.1058, 0.0951, 0.0956,
         0.1063],
        [0.1020, 0.0958, 0.0951, 0.1031, 0.1015, 0.1024, 0.1016, 0.0977, 0.0979,
         0.1029],
        [0.0990, 0.0932, 0.0929, 0.1041, 0.1032, 0.1049, 0.1024, 0.0995, 0.0969,
         0.1039],
        [0.0995, 0.0936, 0.0930, 0.1038, 0.1056, 0.1049, 0.1030, 0.0973, 0.0965,
         0.1029],
        [0.1014, 0.0943, 0.0938, 0.1047, 0.1033, 0.1037, 0.1040, 0.0960, 0.0951,
         0.1038]], device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1036, 0.0992, 0.0976, 0.0994, 0.1035, 0.0968, 0.1014, 0.0978, 0.0964,
         0.1043],
        [0.1029, 0.0988, 0.0985, 0.0998, 0.1064, 0.0984, 0.1004, 0.0970, 0.0987,
         0.0991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1023, 0.0987, 0.1000, 0.0985, 0.0986, 0.0983, 0.1008, 0.1013, 0.0981,
         0.1034],
        [0.1020, 0.0980, 0.1001, 0.1001, 0.1004, 0.0986, 0.1005, 0.0989, 0.0994,
         0.1020],
        [0.1024, 0.0903, 0.0933, 0.1020, 0.1051, 0.1032, 0.1042, 0.1023, 0.0995,
         0.0978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1020, 0.0991, 0.0989, 0.0987, 0.1001, 0.1007, 0.1015, 0.1001, 0.0961,
         0.1029],
        [0.1008, 0.0975, 0.0989, 0.1000, 0.1030, 0.0996, 0.1015, 0.1000, 0.0989,
         0.1000],
        [0.1035, 0.0905, 0.0946, 0.1022, 0.1047, 0.1000, 0.1067, 0.0988, 0.0996,
         0.0994],
        [0.1037, 0.0922, 0.0952, 0.1005, 0.1048, 0.1024, 0.1024, 0.1018, 0.1004,
         0.0966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1019, 0.0980, 0.0981, 0.1017, 0.1007, 0.1006, 0.0977, 0.0983, 0.0992,
         0.1038],
        [0.1014, 0.0977, 0.0983, 0.0989, 0.1020, 0.0991, 0.1020, 0.0991, 0.0986,
         0.1030],
        [0.1022, 0.0906, 0.0935, 0.1027, 0.1059, 0.1036, 0.1032, 0.0977, 0.0994,
         0.1011],
        [0.1034, 0.0927, 0.0948, 0.1009, 0.1025, 0.1035, 0.1020, 0.1005, 0.1015,
         0.0982],
        [0.1042, 0.0940, 0.0957, 0.1000, 0.1027, 0.1003, 0.1023, 0.1018, 0.1021,
         0.0971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 03:40:00 PM | Train: [ 2/3] Step 000/937 Loss 0.468 Prec@(1,5) (84.4%, 100.0%)
04/26 03:41:31 PM | Train: [ 2/3] Step 050/937 Loss 0.557 Prec@(1,5) (80.1%, 99.5%)
04/26 03:43:04 PM | Train: [ 2/3] Step 100/937 Loss 0.526 Prec@(1,5) (80.3%, 99.7%)
04/26 03:44:35 PM | Train: [ 2/3] Step 150/937 Loss 0.524 Prec@(1,5) (80.3%, 99.6%)
04/26 03:46:06 PM | Train: [ 2/3] Step 200/937 Loss 0.509 Prec@(1,5) (81.0%, 99.6%)
04/26 03:47:39 PM | Train: [ 2/3] Step 250/937 Loss 0.503 Prec@(1,5) (81.1%, 99.6%)
04/26 03:49:10 PM | Train: [ 2/3] Step 300/937 Loss 0.499 Prec@(1,5) (81.2%, 99.6%)
04/26 03:50:41 PM | Train: [ 2/3] Step 350/937 Loss 0.492 Prec@(1,5) (81.4%, 99.6%)
04/26 03:52:13 PM | Train: [ 2/3] Step 400/937 Loss 0.488 Prec@(1,5) (81.6%, 99.6%)
04/26 03:53:45 PM | Train: [ 2/3] Step 450/937 Loss 0.483 Prec@(1,5) (81.8%, 99.6%)
04/26 03:55:16 PM | Train: [ 2/3] Step 500/937 Loss 0.481 Prec@(1,5) (82.0%, 99.6%)
04/26 03:56:47 PM | Train: [ 2/3] Step 550/937 Loss 0.481 Prec@(1,5) (82.0%, 99.6%)
04/26 03:58:19 PM | Train: [ 2/3] Step 600/937 Loss 0.478 Prec@(1,5) (82.0%, 99.6%)
04/26 03:59:50 PM | Train: [ 2/3] Step 650/937 Loss 0.475 Prec@(1,5) (82.1%, 99.6%)
04/26 04:01:21 PM | Train: [ 2/3] Step 700/937 Loss 0.476 Prec@(1,5) (82.2%, 99.7%)
04/26 04:02:52 PM | Train: [ 2/3] Step 750/937 Loss 0.477 Prec@(1,5) (82.2%, 99.6%)
04/26 04:04:25 PM | Train: [ 2/3] Step 800/937 Loss 0.475 Prec@(1,5) (82.3%, 99.6%)
04/26 04:05:57 PM | Train: [ 2/3] Step 850/937 Loss 0.474 Prec@(1,5) (82.4%, 99.6%)
04/26 04:07:28 PM | Train: [ 2/3] Step 900/937 Loss 0.472 Prec@(1,5) (82.5%, 99.6%)
04/26 04:08:35 PM | Train: [ 2/3] Step 937/937 Loss 0.472 Prec@(1,5) (82.4%, 99.6%)
04/26 04:08:35 PM | Train: [ 2/3] Final Prec@1 82.4400%
04/26 04:08:36 PM | Valid: [ 2/3] Step 000/937 Loss 0.296 Prec@(1,5) (87.5%, 100.0%)
04/26 04:08:43 PM | Valid: [ 2/3] Step 050/937 Loss 0.428 Prec@(1,5) (85.2%, 99.7%)
04/26 04:08:51 PM | Valid: [ 2/3] Step 100/937 Loss 0.439 Prec@(1,5) (83.7%, 99.7%)
04/26 04:08:58 PM | Valid: [ 2/3] Step 150/937 Loss 0.444 Prec@(1,5) (83.4%, 99.6%)
04/26 04:09:05 PM | Valid: [ 2/3] Step 200/937 Loss 0.438 Prec@(1,5) (83.5%, 99.7%)
04/26 04:09:12 PM | Valid: [ 2/3] Step 250/937 Loss 0.437 Prec@(1,5) (83.5%, 99.7%)
04/26 04:09:20 PM | Valid: [ 2/3] Step 300/937 Loss 0.435 Prec@(1,5) (83.7%, 99.7%)
04/26 04:09:27 PM | Valid: [ 2/3] Step 350/937 Loss 0.438 Prec@(1,5) (83.4%, 99.7%)
04/26 04:09:35 PM | Valid: [ 2/3] Step 400/937 Loss 0.435 Prec@(1,5) (83.7%, 99.7%)
04/26 04:09:42 PM | Valid: [ 2/3] Step 450/937 Loss 0.432 Prec@(1,5) (83.8%, 99.7%)
04/26 04:09:50 PM | Valid: [ 2/3] Step 500/937 Loss 0.436 Prec@(1,5) (83.7%, 99.7%)
04/26 04:09:58 PM | Valid: [ 2/3] Step 550/937 Loss 0.442 Prec@(1,5) (83.6%, 99.7%)
04/26 04:10:05 PM | Valid: [ 2/3] Step 600/937 Loss 0.442 Prec@(1,5) (83.5%, 99.7%)
04/26 04:10:12 PM | Valid: [ 2/3] Step 650/937 Loss 0.442 Prec@(1,5) (83.6%, 99.7%)
04/26 04:10:20 PM | Valid: [ 2/3] Step 700/937 Loss 0.442 Prec@(1,5) (83.6%, 99.7%)
04/26 04:10:27 PM | Valid: [ 2/3] Step 750/937 Loss 0.444 Prec@(1,5) (83.6%, 99.7%)
04/26 04:10:35 PM | Valid: [ 2/3] Step 800/937 Loss 0.446 Prec@(1,5) (83.5%, 99.7%)
04/26 04:10:42 PM | Valid: [ 2/3] Step 850/937 Loss 0.448 Prec@(1,5) (83.4%, 99.7%)
04/26 04:10:49 PM | Valid: [ 2/3] Step 900/937 Loss 0.448 Prec@(1,5) (83.4%, 99.7%)
04/26 04:10:55 PM | Valid: [ 2/3] Step 937/937 Loss 0.449 Prec@(1,5) (83.3%, 99.7%)
04/26 04:10:55 PM | Valid: [ 2/3] Final Prec@1 83.3433%
04/26 04:10:55 PM | genotype = Genotype(normal=[[('sep_conv_5x5', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 0), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 0), ('dil_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('mb_conv_3x3', 0)], [('sep_conv_5x5', 2), ('sep_conv_5x5', 3)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0983, 0.0922, 0.0910, 0.1060, 0.1085, 0.1025, 0.1047, 0.0949, 0.0928,
         0.1090],
        [0.1067, 0.0944, 0.0935, 0.1058, 0.1019, 0.0991, 0.1010, 0.0993, 0.0983,
         0.0999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0980, 0.0944, 0.0942, 0.1029, 0.1042, 0.1016, 0.1034, 0.1013, 0.0939,
         0.1063],
        [0.1051, 0.0937, 0.0917, 0.1000, 0.1080, 0.0993, 0.1031, 0.0991, 0.0968,
         0.1032],
        [0.1018, 0.0921, 0.0906, 0.1013, 0.1018, 0.1011, 0.1091, 0.1011, 0.0987,
         0.1025]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0961, 0.0918, 0.0921, 0.1027, 0.1093, 0.1025, 0.1038, 0.0967, 0.0970,
         0.1081],
        [0.1030, 0.0949, 0.0939, 0.1018, 0.1058, 0.1027, 0.1030, 0.0973, 0.0945,
         0.1033],
        [0.0994, 0.0909, 0.0901, 0.1086, 0.1075, 0.1034, 0.1025, 0.0976, 0.0964,
         0.1036],
        [0.0993, 0.0921, 0.0902, 0.1070, 0.1070, 0.1049, 0.1028, 0.0974, 0.0978,
         0.1015]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0966, 0.0927, 0.0931, 0.1054, 0.1050, 0.1052, 0.1076, 0.0926, 0.0937,
         0.1080],
        [0.1045, 0.0948, 0.0925, 0.1025, 0.1026, 0.1021, 0.1004, 0.1002, 0.0977,
         0.1027],
        [0.1006, 0.0924, 0.0909, 0.1049, 0.1045, 0.1049, 0.1017, 0.1003, 0.0971,
         0.1028],
        [0.1015, 0.0935, 0.0913, 0.1035, 0.1058, 0.1069, 0.1009, 0.0977, 0.0971,
         0.1017],
        [0.1042, 0.0949, 0.0936, 0.1048, 0.1037, 0.1035, 0.1013, 0.0965, 0.0954,
         0.1021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)

# Alpha - reduce
tensor([[0.1035, 0.0977, 0.0947, 0.1000, 0.1066, 0.0951, 0.1006, 0.0993, 0.0945,
         0.1079],
        [0.1037, 0.0976, 0.0978, 0.0995, 0.1096, 0.0992, 0.0995, 0.0977, 0.0965,
         0.0990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.1011, 0.0960, 0.0995, 0.0983, 0.0985, 0.0978, 0.1024, 0.1039, 0.0966,
         0.1058],
        [0.1019, 0.0955, 0.0980, 0.1013, 0.1002, 0.0976, 0.1004, 0.1024, 0.0999,
         0.1030],
        [0.1033, 0.0837, 0.0878, 0.1036, 0.1089, 0.1037, 0.1081, 0.1029, 0.1018,
         0.0962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0992, 0.0963, 0.0980, 0.0976, 0.1033, 0.1015, 0.1019, 0.1004, 0.0968,
         0.1050],
        [0.1007, 0.0957, 0.0990, 0.1018, 0.1027, 0.0978, 0.0998, 0.1023, 0.1001,
         0.1001],
        [0.1043, 0.0848, 0.0910, 0.1037, 0.1089, 0.0999, 0.1072, 0.1000, 0.1000,
         0.1002],
        [0.1054, 0.0872, 0.0915, 0.1010, 0.1079, 0.1025, 0.1041, 0.1045, 0.1016,
         0.0943]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
tensor([[0.0993, 0.0956, 0.0999, 0.1050, 0.1028, 0.1012, 0.0939, 0.0982, 0.0983,
         0.1058],
        [0.1006, 0.0960, 0.0995, 0.0994, 0.1034, 0.0966, 0.1024, 0.1004, 0.0983,
         0.1034],
        [0.1024, 0.0843, 0.0898, 0.1034, 0.1105, 0.1079, 0.1045, 0.0970, 0.0974,
         0.1029],
        [0.1056, 0.0879, 0.0916, 0.1002, 0.1053, 0.1053, 0.1046, 0.1003, 0.1025,
         0.0967],
        [0.1064, 0.0895, 0.0931, 0.1003, 0.1056, 0.1009, 0.1031, 0.1043, 0.1010,
         0.0958]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
#####################
04/26 04:10:59 PM | Train: [ 3/3] Step 000/937 Loss 0.268 Prec@(1,5) (90.6%, 100.0%)
04/26 04:12:30 PM | Train: [ 3/3] Step 050/937 Loss 0.432 Prec@(1,5) (84.7%, 99.8%)
04/26 04:14:01 PM | Train: [ 3/3] Step 100/937 Loss 0.415 Prec@(1,5) (85.0%, 99.7%)
04/26 04:15:33 PM | Train: [ 3/3] Step 150/937 Loss 0.414 Prec@(1,5) (84.9%, 99.7%)
04/26 04:17:04 PM | Train: [ 3/3] Step 200/937 Loss 0.413 Prec@(1,5) (85.0%, 99.7%)
04/26 04:18:36 PM | Train: [ 3/3] Step 250/937 Loss 0.422 Prec@(1,5) (84.7%, 99.7%)
04/26 04:20:08 PM | Train: [ 3/3] Step 300/937 Loss 0.427 Prec@(1,5) (84.4%, 99.7%)
04/26 04:21:40 PM | Train: [ 3/3] Step 350/937 Loss 0.418 Prec@(1,5) (84.7%, 99.8%)
04/26 04:23:11 PM | Train: [ 3/3] Step 400/937 Loss 0.421 Prec@(1,5) (84.8%, 99.8%)
04/26 04:24:43 PM | Train: [ 3/3] Step 450/937 Loss 0.419 Prec@(1,5) (84.8%, 99.7%)
04/26 04:26:14 PM | Train: [ 3/3] Step 500/937 Loss 0.415 Prec@(1,5) (85.0%, 99.8%)
04/26 04:27:46 PM | Train: [ 3/3] Step 550/937 Loss 0.414 Prec@(1,5) (85.1%, 99.7%)
04/26 04:29:17 PM | Train: [ 3/3] Step 600/937 Loss 0.412 Prec@(1,5) (85.2%, 99.7%)
04/26 04:30:49 PM | Train: [ 3/3] Step 650/937 Loss 0.410 Prec@(1,5) (85.2%, 99.8%)
04/26 04:32:22 PM | Train: [ 3/3] Step 700/937 Loss 0.408 Prec@(1,5) (85.3%, 99.8%)
04/26 04:33:53 PM | Train: [ 3/3] Step 750/937 Loss 0.406 Prec@(1,5) (85.4%, 99.8%)
04/26 04:35:24 PM | Train: [ 3/3] Step 800/937 Loss 0.406 Prec@(1,5) (85.3%, 99.8%)
04/26 04:36:56 PM | Train: [ 3/3] Step 850/937 Loss 0.404 Prec@(1,5) (85.4%, 99.8%)
04/26 04:38:28 PM | Train: [ 3/3] Step 900/937 Loss 0.404 Prec@(1,5) (85.4%, 99.8%)
04/26 04:39:34 PM | Train: [ 3/3] Step 937/937 Loss 0.404 Prec@(1,5) (85.4%, 99.8%)
04/26 04:39:34 PM | Train: [ 3/3] Final Prec@1 85.3700%
04/26 04:39:35 PM | Valid: [ 3/3] Step 000/937 Loss 0.283 Prec@(1,5) (90.6%, 100.0%)
04/26 04:39:42 PM | Valid: [ 3/3] Step 050/937 Loss 0.393 Prec@(1,5) (85.4%, 99.6%)
04/26 04:39:50 PM | Valid: [ 3/3] Step 100/937 Loss 0.396 Prec@(1,5) (85.5%, 99.7%)
04/26 04:39:57 PM | Valid: [ 3/3] Step 150/937 Loss 0.389 Prec@(1,5) (85.6%, 99.7%)
04/26 04:40:04 PM | Valid: [ 3/3] Step 200/937 Loss 0.396 Prec@(1,5) (85.3%, 99.7%)
04/26 04:40:11 PM | Valid: [ 3/3] Step 250/937 Loss 0.389 Prec@(1,5) (85.5%, 99.7%)
04/26 04:40:19 PM | Valid: [ 3/3] Step 300/937 Loss 0.386 Prec@(1,5) (85.8%, 99.7%)
04/26 04:40:27 PM | Valid: [ 3/3] Step 350/937 Loss 0.385 Prec@(1,5) (85.6%, 99.8%)
04/26 04:40:33 PM | Valid: [ 3/3] Step 400/937 Loss 0.388 Prec@(1,5) (85.5%, 99.7%)
04/26 04:40:41 PM | Valid: [ 3/3] Step 450/937 Loss 0.388 Prec@(1,5) (85.6%, 99.7%)
04/26 04:40:48 PM | Valid: [ 3/3] Step 500/937 Loss 0.391 Prec@(1,5) (85.5%, 99.7%)
04/26 04:40:55 PM | Valid: [ 3/3] Step 550/937 Loss 0.389 Prec@(1,5) (85.7%, 99.7%)
04/26 04:41:02 PM | Valid: [ 3/3] Step 600/937 Loss 0.391 Prec@(1,5) (85.6%, 99.7%)
04/26 04:41:10 PM | Valid: [ 3/3] Step 650/937 Loss 0.392 Prec@(1,5) (85.5%, 99.7%)
04/26 04:41:17 PM | Valid: [ 3/3] Step 700/937 Loss 0.390 Prec@(1,5) (85.6%, 99.7%)
04/26 04:41:24 PM | Valid: [ 3/3] Step 750/937 Loss 0.390 Prec@(1,5) (85.5%, 99.7%)
04/26 04:41:32 PM | Valid: [ 3/3] Step 800/937 Loss 0.391 Prec@(1,5) (85.6%, 99.7%)
04/26 04:41:38 PM | Valid: [ 3/3] Step 850/937 Loss 0.390 Prec@(1,5) (85.6%, 99.7%)
04/26 04:41:46 PM | Valid: [ 3/3] Step 900/937 Loss 0.391 Prec@(1,5) (85.6%, 99.7%)
04/26 04:41:51 PM | Valid: [ 3/3] Step 937/937 Loss 0.391 Prec@(1,5) (85.6%, 99.7%)
04/26 04:41:51 PM | Valid: [ 3/3] Final Prec@1 85.6233%
04/26 04:41:51 PM | genotype = Genotype(normal=[[('sep_conv_5x5', 0), ('max_pool_3x3', 1)], [('sep_conv_5x5', 1), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 0), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 0), ('sep_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('mb_conv_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 0)]], reduce_concat=range(2, 6))
04/26 04:41:52 PM | Final best Prec@1 = 85.6233%
04/26 04:41:52 PM | Best Genotype = Genotype(normal=[[('sep_conv_5x5', 0), ('max_pool_3x3', 1)], [('sep_conv_5x5', 1), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 0), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 0), ('sep_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('mb_conv_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 0)]], reduce_concat=range(2, 6))
